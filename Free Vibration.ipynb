{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving free vibration using PINNs\n",
    "Considering: M = 1 kg , C = 1 N.s/m , K = 1 N/m "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from smt.sampling_methods import LHS\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time as timelib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = timelib.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_sol(t):\n",
    "    o = 0.5*torch.exp(-t/2)*(torch.sin(torch.sqrt(torch.tensor(3))*t/2)/torch.sqrt(torch.tensor(3)) + torch.cos(torch.sqrt(torch.tensor(3))*t/2))\n",
    "    return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the domain intervals for each dimension\n",
    "xlimits = np.array([[0.0, 10.0],[0.0,0.0]])\n",
    "\n",
    "# Create an LHS sampling instance\n",
    "sampling = LHS(xlimits=xlimits)\n",
    "\n",
    "# Generate 10000 samples\n",
    "num_samples = 1000\n",
    "samples = sampling(num_samples)\n",
    "\n",
    "# Visualize the samples\n",
    "plt.plot(samples[:, 0],samples[:,1], \"o\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Generated {num_samples} samples with LHS.\")\n",
    "# t = np.linspace(0,1,100)\n",
    "\n",
    "# x = np.ones_like(t) * 0.0\n",
    "x = samples[:, 0]\n",
    "x = torch.from_numpy(x)\n",
    "x = x.float().to(device)\n",
    "x.requires_grad = True\n",
    "\n",
    "x_init = torch.tensor(0.0,dtype=torch.float32,requires_grad=True).to(device)\n",
    "y_init = torch.tensor(0.5,dtype=torch.float32,requires_grad=True).to(device)\n",
    "dy_init = torch.tensor(0.0,dtype=torch.float32,requires_grad=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN(nn.Module):\n",
    "    def __init__(self,layers) -> None:\n",
    "        super(PINN,self).__init__()\n",
    "        self\n",
    "\n",
    "        self.x = x\n",
    "        self.x_init = x_init\n",
    "        self.y_init = y_init\n",
    "        self.dy_init = dy_init\n",
    "\n",
    "        self.loss_rec = []\n",
    "\n",
    "        self.layers = layers\n",
    "        self.net = nn.Sequential()\n",
    "        for i in range(len(layers) - 2):\n",
    "            self.net.add_module(f'layer_{i}', nn.Linear(layers[i], layers[i+1]))\n",
    "            self.net.add_module(f'activation_{i}', nn.Tanh())\n",
    "            # self.net.add_module(f'dropout_{i}', nn.Dropout(p=self.dropout_prob))  # Add dropout\n",
    "            # self.net.add_module(f'batchnorm_{i}', nn.BatchNorm1d(layers[i+1])) # Add batchnorm\n",
    "        self.net.add_module('output', nn.Linear(layers[-2], layers[-1]))\n",
    "        self.net.add_module(f'activation_output', nn.Tanh())\n",
    "\n",
    "        self.adam = torch.optim.Adam(self.net.parameters(),lr=5e-3)\n",
    "        self.lbfgs = torch.optim.LBFGS(\n",
    "                                        self.net.parameters(),\n",
    "                                        lr=1,\n",
    "                                        max_iter=2000,\n",
    "                                        max_eval=2000,\n",
    "                                        tolerance_grad=0,\n",
    "                                        tolerance_change=0,\n",
    "                                        history_size=500,\n",
    "                                        line_search_fn=\"strong_wolfe\",\n",
    "                                       )\n",
    "    def forward(self,x):\n",
    "        u = self.net(x.view(-1,1))\n",
    "        return u[:, 0]\n",
    "    \n",
    "    def ODE_loss(self):\n",
    "        y = self.forward(self.x)\n",
    "        dy = torch.autograd.grad(y.sum(),self.x , create_graph=True)[0]\n",
    "        d2y = torch.autograd.grad(dy.sum(),self.x, create_graph=True)[0]\n",
    "        return torch.mean(torch.square(d2y + dy + y))\n",
    "    \n",
    "    def boundary_loss(self):\n",
    "        y = self.forward(self.x_init)\n",
    "        dy = torch.autograd.grad(y.sum(),self.x_init , create_graph=True)[0]\n",
    "       \n",
    "        return torch.mean(torch.square(y-self.y_init)) + torch.mean(torch.square(dy-self.dy_init))\n",
    "    \n",
    "    def closure(self):\n",
    "        self.adam.zero_grad()\n",
    "        self.lbfgs.zero_grad()\n",
    "\n",
    "        loss = self.boundary_loss() + self.ODE_loss()\n",
    "        self.loss_rec.append(loss.detach().cpu().item())\n",
    "\n",
    "        print(f\"\\r epoch {len(self.loss_rec)} , loss : {loss.detach().cpu().item():5e} , time : {timelib.time()-st:.2f} s\",end=\"\",)\n",
    "        if len(self.loss_rec)%100 ==0:\n",
    "            print(\"\")\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def train(self,optimizer,epoch):\n",
    "        try:\n",
    "            c = 0\n",
    "            for i in optimizer:\n",
    "                for j in range(epoch[c]):\n",
    "                    if i == self.adam:\n",
    "                        ls = self.closure()\n",
    "                        i.step()\n",
    "                    else:\n",
    "                        i.step(self.closure)\n",
    "                \n",
    "                c+=1\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\")\n",
    "            print(\"intrrupted by user\")\n",
    "\n",
    "    def plot(self):\n",
    "        with torch.no_grad():\n",
    "            plt.figure(figsize=(5,2.5))\n",
    "            plt.semilogy(range(len(self.loss_rec)),self.loss_rec)\n",
    "            plt.ylim([0, max(self.loss_rec)])\n",
    "            plt.xlim([0,len(self.loss_rec)])\n",
    "            plt.ylabel('Loss')\n",
    "            plt.xlabel('Epoch')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ode_solver = PINN([1,8,8,8,1]).to(device)\n",
    "optimizer = [ode_solver.adam,ode_solver.lbfgs]\n",
    "epoch = [100,200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ode_solver.train(optimizer,epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ode_solver.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = torch.linspace(0,10,1000).to(device)\n",
    "y = ode_solver(l)\n",
    "y_ex = exact_sol(l)\n",
    "plt.plot(l.detach().cpu().numpy(),y.detach().cpu().numpy())\n",
    "plt.plot(l.detach().cpu().numpy(),y_ex.detach().cpu().numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
